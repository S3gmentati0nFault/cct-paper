\externaldocument{../main.tex}

In this chapter I will present the countless opportunities that come from a merge between AI and the
upcoming 6G network. Clearly the opportunities depend on the architecture of the network and are
extremely example specific but I am going to try and condense the structure of the various models
proposed in the very recent literature into a series of shared features.

The idea of implementing AI as an agent in the field of 6G networking is rooted into the want for
the creation of the most versatile personal assistant in commerce, attempts have been made in the
past on the consumer level with clearly inferior models (e.g. "Siri" or "Cortana") but now the
objective is to create a seamless and encompassing ecosystem that can adapt and provide the user
with information on his own self that goes beyond anything we can currently imagine. Considering the
litreature that is looking to the future the most the hypothesis of developing a completely
autonomous edge AI capable of self-improvement and self-organization \cite{ai4ci} is a realistic
prospect; forgetting for a moment all of the complex problems that need to be addressed it does look
like a problem that can be solved with the materials at hand.

The question that I will try to answer now is what is required to have a network that can handle
users, provide them with very custom experiences and react to situations and stimuli automatically
without the need of human intervention.

\bigskip
\noindent
END:
\phantomsection
\label{ssec:end}

Going from the ground-up we can build the network architecture starting with the sensors, the end
devices, basically a fog of computational devices of all matters and sizes that are able to do
computation, generate data or process information.

Of extreme importance next to the controller, the end devices are the entities that provide much of
the data that allow the controller LLM to do inference, like in the case explored in the \cite{pga}
paper the data necessary to create a description of a car crash or an incident is provided by a set
of sensors that constantly generate data and are able to do inference on the generated data using
very limited LLMs that are useful to be able to do initial assessments of situations.

\bigskip
\noindent
CONTROLLER:
\phantomsection
\label{ssec:controller}

As was said in previous sections AI models have come very far thanks to the rapid improvements in
the field of LLM development. It's easy to understand that, if it's necessary to provide the user
with a very custom experience it's also necessary to have a system that can understand both the
context and the necessity without the need of additional information (since the premise is that the
system has to be integrated seamlessly with the user's life).
In some cases the LLM controller was just considered to be a larger downstream node inside the
network capable of handling the aggregation of the results of other smaller models like in
\cite{pga}, and it was used as an example for automatic crash detection and reconstruction in the
field of autonomous driving.
In other cases the LLM controller is used to understand the user's necessities and modifies the
system accordingly, like in \cite{ai4ci}. Another solution seems to be more suited for a RLM
approach, using a RLM trained to do efficient dispatching of resources \cite{llm6G}.

In any way, shape, or form the controller, usually named also edge, is shipped it's a necessary
component of a network such as 6G because it's going to be necessary to be able to understand and
act according to the user's status and context. It also provides more computational power for bigger
inference tasks that would be otherwise impossible on the smaller end nodes.

If we consider a more industry-driven example, even in the field of heavy machinery or energy
production, it's very useful to have a system able to take a decision extremely rapidly based on a
certain situation. Modern control systems are able to take action and make corrections rapidly and
precisely, providing excellent results and avoiding costly mistakes, having an LLM that is instead able
to make more "management-oriented" decisions based on data might provide itself useful to avoid
energy or time wastes.

\bigskip
\noindent
CLOUD:
\phantomsection
\label{ssec:cloud}

Last but not least it's important to take into consideration the fact that the bigger models that
are used as a foundation for all of the other models that can be deployed through the controller to
allow the user to have the most comprehensive and cohesive experience possible need to be trained
somehow.

If we consider the GPT-3 model, which is the one that is the name that appears the most in the
recent literature, it would be necessary to have more than 1000 GPUs running non-stop for more than
4 months to have the complete training of the 175B parameters that compose the model \cite{gaisnet}. This task
would be impossible for the low power systems deployed at the end or even at the edge, therefore
it's necessary to leverage the power of big data-centers positioned at the end of the network
hierarchy. It's important to note, obviously, that such a task would be a waste of time for the big
data-centers that we place at the end of the hierarchy, I will provide more detailed information
about the various techniques that have been taken into consideration to solve the problem of
handling enormous models in reduced spaces.

\bigskip
I have to admit that from what I had originally promised this section of the paper does look way
more bland than originally intended, this architecture that I provided above is nothing new and we
have already seen it many times and is just the normal evolution of the cloud environment (which is
already partly taking place at the moment), and I think that is the reason why it's extremely
effective, the architecture is nothing new or revolutionary, it's just the fusion (with some evident
caveats that we'll go through in the next session) of artificial intelligence with the pre-existing
cloud and wireless-network infrastructure.

In the next session I will go through the various challenges that we will be facing when the moment
comes to start implementing the fusion of the service architecture of 5G with the AI agents and why
the upgrade is not as easy as it might look like in the above explanation.
